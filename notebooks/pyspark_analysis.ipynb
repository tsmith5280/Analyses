# Cell 1: Setup Spark
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("YourProject").getOrCreate()

# Cell 2: Load data
df = spark.read.csv("../data/raw/your_dataset.csv", header=True, inferSchema=True)
df.printSchema()
df.show(5)

# Cell 3: Cleaning
df = df.na.drop()

# Cell 4: GroupBy / Aggregation
from pyspark.sql.functions import sum
df.groupBy("region").agg(sum("population").alias("total_pop")).show()

# Cell 5: Window Functions
from pyspark.sql.window import Window
from pyspark.sql.functions import rank

windowSpec = Window.partitionBy("region").orderBy(df["population"].desc())
df.withColumn("rank", rank().over(windowSpec)).show()

# Cell 6: Export
df.write.csv("../results/output.csv", header=True)
